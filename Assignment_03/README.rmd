---
title: "Assignment 3"
author: "Christina Lin"
date: "10/29/2021"
output:
  github_document: default
  html_document:
    html_preview: false
  word_document: default
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "http://cran.rstudio.com"))
library(tidyverse)
library(tidytext)
library(xml2)
library(httr)
library(stringr)
```

# Part 1: APIs


```{r}
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2+trial+vaccine")

# Finding the number of papers
counts <- xml2::xml_find_first(website, xpath = "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")

counts <- as.character(counts)
counts <- stringr::str_extract(counts, "[[:digit:],]+")
counts <- stringr::str_remove_all(counts, ",")
counts <- as.integer(counts)
```

When searched on the PubMed website with the term "sars-cov-2 trial vaccine", there are `r counts` results.


## Using the NCBI API

```{r}
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(
    db = "pubmed",
    term = "sars-cov-2 trial vaccine",
    retmax = "5000"
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)

# Turn the result into a character vector
ids <- as.character(ids)

# Find all the ids 
ids <- stringr::str_extract_all(ids, "<Id>[[:digit:]]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")

```

Searching the NCBI API returns `r length(ids)` UIDs for publications associated with "sars-cov-2 trial vaccine". 


Now getting the details for the first 250 papers. 
```{r}
# Getting the abstract of the papers

publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/",
  path = "entrez/eutils/efetch.fcgi",
  query = list(
    db = "pubmed",
    id = I(paste(ids[1:250], collapse=",")),
    retmax = "250",
    rettype = "abstract"
    )
)

# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)

```


Forming a database: 

```{r}
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
```


Getting the titles
```{r}
titles <- str_extract(pub_char_list, "<ArticleTitle>[[:print:][:space:]]+</ArticleTitle>")
titles <- str_remove_all(titles, "</?[[:alnum:][:punct:]]+>")
```


Getting the journals
```{r}
journals <- str_extract(pub_char_list, "<Journal>[[:print:][:space:]]+</Journal>")
journals <- str_extract(journals,"<Title>[[:print:][:space:]]+</Title>")
journals <- str_remove_all(journals, "</?[[:alnum:][:punct:]]+>")

```

Getting the publication date

```{r}
dates <- str_extract(pub_char_list, "<PubDate>[[:print:][:space:]]+</PubDate>")
dates <- str_replace_all(dates, "</Year>\\n", "-")
dates <- str_replace_all(dates, "</Month>\\n", "-")
dates <- str_replace_all(dates, "</Day>\\n", "")
dates <- str_remove_all(dates, "</?[[:alnum:][:space:]]+>")
dates <- str_remove_all(dates, "[:space:]+")
```


Getting the abstracts and cleaning them up
```{r}
abstracts <- str_extract(pub_char_list, "<Abstract>[[:print:][:space:]]+</Abstract>")
abstracts <- str_remove_all(abstracts, "</?[[:alnum:][:punct:]]+>")
abstracts <- str_replace_all(abstracts, "[[:space:]]+", " ")
```


Finally, the dataset

```{r}
database <- data.frame(
  PubMedID = ids[1:250],
  Title = titles,
  Journal = journals,
  Date = dates,
  Abstracts = abstracts
)
knitr::kable(database[1:20,], caption = "First 20 papers about sars-cov-2 trial vaccine")
```

# Part 2: Text Mining

