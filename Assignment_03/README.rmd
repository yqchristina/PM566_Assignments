---
title: "Assignment 3"
author: "Christina Lin"
date: "10/29/2021"
output:
  github_document: default
  html_document:
    html_preview: false
  word_document: default
always_allow_html: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(repos = c(CRAN = "http://cran.rstudio.com"))
library(tidyverse)
library(tidytext)
library(xml2)
library(httr)
library(stringr)
library(dplyr)
library(ggplot2)
```

# Part 1: APIs


```{r}
# Downloading the website
website <- xml2::read_html("https://pubmed.ncbi.nlm.nih.gov/?term=sars-cov-2+trial+vaccine")

# Finding the number of papers
counts <- xml2::xml_find_first(website, xpath = "/html/body/main/div[9]/div[2]/div[2]/div[1]/div[1]/span")

counts <- as.character(counts)
counts <- stringr::str_extract(counts, "[[:digit:],]+")
counts <- stringr::str_remove_all(counts, ",")
counts <- as.integer(counts)
```

When searched on the PubMed website with the term "sars-cov-2 trial vaccine", there are `r counts` results.


## Using the NCBI API

```{r}
query_ids <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi",
  query = list(
    db = "pubmed",
    term = "sars-cov-2 trial vaccine",
    retmax = "5000"
)
)
# Extracting the content of the response of GET
ids <- httr::content(query_ids)

# Turn the result into a character vector
ids <- as.character(ids)

# Find all the ids 
ids <- stringr::str_extract_all(ids, "<Id>[[:digit:]]+</Id>")[[1]]
# Remove all the leading and trailing <Id> </Id>. Make use of "|"
ids <- stringr::str_remove_all(ids, "<Id>|</Id>")

```

Searching the NCBI API returns `r length(ids)` UIDs for publications associated with "sars-cov-2 trial vaccine". 


Now getting the details for the first 250 papers. 
```{r}
# Getting the abstract of the papers

publications <- GET(
  url   = "https://eutils.ncbi.nlm.nih.gov/",
  path = "entrez/eutils/efetch.fcgi",
  query = list(
    db = "pubmed",
    id = I(paste(ids[1:250], collapse=",")),
    retmax = "250",
    rettype = "abstract"
    )
)

# Turning the output into character vector
publications <- httr::content(publications)
publications_txt <- as.character(publications)

```


Forming a database: 

```{r}
pub_char_list <- xml2::xml_children(publications)
pub_char_list <- sapply(pub_char_list, as.character)
```


Getting the titles
```{r}
titles <- str_extract(pub_char_list, "<ArticleTitle>[[:print:][:space:]]+</ArticleTitle>")
titles <- str_remove_all(titles, "</?[[:alnum:][:punct:]]+>")
```


Getting the journals
```{r}
journals <- str_extract(pub_char_list, "<Journal>[[:print:][:space:]]+</Journal>")
journals <- str_extract(journals,"<Title>[[:print:][:space:]]+</Title>")
journals <- str_remove_all(journals, "</?[[:alnum:][:punct:]]+>")

```

Getting the publication date

```{r}
dates <- str_extract(pub_char_list, "<PubDate>[[:print:][:space:]]+</PubDate>")
dates <- str_replace_all(dates, "</Year>\\n", "-")
dates <- str_replace_all(dates, "</Month>\\n", "-")
dates <- str_replace_all(dates, "</Day>\\n", "")
dates <- str_remove_all(dates, "</?[[:alnum:][:space:]]+>")
dates <- str_remove_all(dates, "[:space:]+")
```


Getting the abstracts and cleaning them up
```{r}
abstracts <- str_extract(pub_char_list, "<Abstract>[[:print:][:space:]]+</Abstract>")
abstracts <- str_remove_all(abstracts, "</?[[:alnum:][:punct:]]+>")
abstracts <- str_replace_all(abstracts, "[[:space:]]+", " ")
```


Finally, the dataset

```{r}
database <- data.frame(
  PubMedID = ids[1:250],
  Title = titles,
  Journal = journals,
  Date = dates,
  Abstracts = abstracts
)
knitr::kable(database[1:5,], caption = "First 5 papers about sars-cov-2 trial vaccine")
```



# Part 2: Text Mining

Downloading the data:
```{r}
if (!file.exists("pubmed.csv"))
  download.file(
    url = "https://raw.githubusercontent.com/USCbiostats/data-science-data/master/03_pubmed/pubmed.csv",
    destfile = "pubmed.csv",
    method   = "libcurl",
    timeout  = 60
    )
pubmed <- data.table::fread("pubmed.csv")

```


## Question 1: Tokenizing the abstracts

```{r}
pubmed %>%
  unnest_tokens(output = token,input = abstract) %>%
  count(token, sort = TRUE) %>%
  top_n(20,n) %>%
  knitr::kable()
```

The most frequent words are stop words in this dataset, but words like "covid", "patients", and "cancer" are also very common. 


Now removing the stop words:
```{r}
pubmed %>%
  unnest_tokens(output = token,input = abstract) %>%
  count(token, sort = TRUE) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  top_n(20,n) %>%
  knitr::kable()
```

After removing the stop words, the most common words are those related to the search terms, as expected.  We also observe "pregnancy" and "women" as common words, as well as other medical terms such as "treatment", "diagnosis", and "clinical". 


Now observing the 5 most common tokens for each search term after removing stop words:

```{r}
pubmed[term=="covid"] %>%
  unnest_tokens(output = token,input = abstract) %>%
  count(token, sort = TRUE) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  top_n(5,n) %>%
  knitr::kable(caption = "5 Most Frequent Tokens in Abstracts with Search Term 'covid'")
```


```{r}
pubmed[term=="cystic fibrosis",] %>%
  unnest_tokens(output = token,input = abstract) %>%
  count(token, sort = TRUE) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  top_n(5,n) %>%
  knitr::kable(caption = "5 Most Frequent Tokens in Abstracts with Search Term 'cystic fibrosis'")
```


```{r}
pubmed[term=="meningitis",] %>%
  unnest_tokens(output = token,input = abstract) %>%
  count(token, sort = TRUE) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  top_n(5,n) %>%
  knitr::kable(caption = "5 Most Frequent Tokens in Abstracts with Search Term 'meningitis'")
```


```{r}
pubmed[term=="preeclampsia",] %>%
  unnest_tokens(output = token,input = abstract) %>%
  count(token, sort = TRUE) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  top_n(5,n) %>%
  knitr::kable(caption = "5 Most Frequent Tokens in Abstracts with Search Term 'preeclampsia'")
```


```{r}
pubmed[term=="prostate cancer",] %>%
  unnest_tokens(output = token,input = abstract) %>%
  count(token, sort = TRUE) %>%
  anti_join(stop_words, by = c("token" = "word")) %>%
  top_n(5,n) %>%
  knitr::kable(caption = "5 Most Frequent Tokens in Abstracts with Search Term 'prostate cancer'")
```


## Question 2: Bi-grams

```{r}
pubmed %>%
  unnest_ngrams(output = bigram,input = abstract, n = 2) %>%
  count(bigram, sort = TRUE) %>%
  top_n(10)%>%
  ggplot(aes(n, fct_reorder(bigram,n))) + 
  geom_col()
```


The most common bi-grams are related to the search terms, with covid 19 being the most frequent. 


## Question 3: TF-IDF Values

Calculating the TF-IDF values for each search term: 

```{r}
pubmed %>%
  unnest_tokens(output = text, input = abstract) %>%
  count(text, term) %>%
  bind_tf_idf(text, term, n) %>%
  arrange(desc(tf_idf)) %>% 
  group_by(term) %>%
  top_n(5,tf_idf) %>%
  arrange(desc(term)) %>%
  knitr::kable()
```
